# -*- coding: utf-8 -*-
"""[NeuralForecast] Telemetrica - estacao 56994500.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qz7CS_JLmkVhhJUcmU-xS5-SPj8FfwYd

# Getting Started
> Fit an LSTM and NHITS model

This notebook provides an example on how to start using the main functionalities of the NeuralForecast library. The `NeuralForecast` class allows users to easily interact with `NeuralForecast.models` PyTorch models. In this example we will forecast AirPassengers data with a classic `LSTM` and the recent `NHITS` models. The full list of available models is available [here](https://nixtla.github.io/neuralforecast/models.html).

You can run these experiments using GPU with Google Colab.

<a href="https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/Getting_Started.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

## 1. Installing NeuralForecast
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install neuralforecast sweetviz sktime pmdarima

# Desativar as mensagens de 'warning' que ficam poluindo o output aqui no Colab

import warnings
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Importações dos módulos necessários para autenticar na minha conta Google e acessar o Drive
# import pandas as pd, numpy as np, matplotlib.pyplot as plt, requests as rt, seaborn as sns
# 
# from IPython.display import display, Markdown
# 
# from io               import BytesIO
# from matplotlib       import style
# from matplotlib.pylab import rcParams
# from pandas.plotting  import register_matplotlib_converters
# 
# from neuralforecast         import NeuralForecast
# from neuralforecast.models  import LSTM, NHITS, RNN, NBEATS
# from neuralforecast.auto    import AutoRNN
# from neuralforecast.utils   import AirPassengersDF
# 
# from keras.models import Sequential
# from keras.layers import LSTM, Dense, Bidirectional
# 
# from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error
# from sklearn.preprocessing import MinMaxScaler
# 
# from sktime.utils.plotting import plot_series
# # from sktime.forecasting.compose import make_reduction
# # from sktime.forecasting.model_selection import temporal_train_test_split
# # from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError
# 
# # Ajustes feitos para geração e criação de gráfico
# style.use('ggplot')
# rcParams['figure.figsize'] = 15, 6
# 
# # Tratar conversões de DateTime entre o Pandas e o Matplotlib
# register_matplotlib_converters()

def sliding_window(data, n_lags=1, n_targets=1, dropnan=True):
	"""
	Rearranja a série temporal como um dataset para aprendizado supervisionado.
	Argumentos:
		data: Sequência de observações. Pode ser list ou NumPy array
		n_lags: Número de lags das observações que serão usadas como input (X).
		n_targets: Número de observações que serão usadas como output (y).
		dropnan: Booleano pra dropar as linhas que ficarem com NaN .
	Retorna:
		Um DataFrame como uma série ajustada para aprendizado supervisionado.
	"""
	n_vars = 1 if type(data) is list else data.shape[1]

	df = pd.DataFrame(data)
	cols, names = list(), list()

	# As lags (t-n, ... t-1)
	for i in range(n_lags, 0, -1):
		cols.append(df.shift(i))
		names += [('X_%d(t-%d)' % (j+1, i)) for j in range(n_vars)]

	# Prever uma sequência (t, t+1, ... t+n)
	for i in range(0, n_targets):
		cols.append(df.shift(-i))
		if i == 0:
			names += [('y_%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('y_%d(t+%d)' % (j+1, i)) for j in range(n_vars)]

	# Junta tudo
	agg = pd.concat(cols, axis=1)
	agg.columns = names

	# Dropa as linhas com NaN
	if dropnan:
		agg.dropna(inplace=True)

	return agg # IMPORTANTE!!!! O retorno sai com as colunas com nome zuado. Tem que renomear o DataFrame resultante depois.

# O corte realizado por esse método é simples: pega do início da série até train_size e coloca num dataframe para treino
# O que resta do corte anterior, é colocado no dataframe para teste
def split_train_test(df, train_size=0.7):

  # Tamanho total da série
  size = df.shape[0]

  # Tamanho do treino
  t_size = int(size * train_size)

  train_data = df.iloc[0:t_size]
  validation_data = df.iloc[t_size:]

  return train_data, validation_data

"""# Carregando os dados de chuva e vazão da estação telemétrica"""

def read_telemetrica_chuva_vazao(codigoEstacao, tipoEstacao, chave):
  link = 'https://docs.google.com/spreadsheets/d/' + chave + '/export?format=xlsx'
  r = rt.get(link)
  data = r.content
  df = pd.read_excel(BytesIO(data), sheet_name='telemetrica_56994500', header=0, parse_dates=['data_hora'])

  # Ajustando a coluna correspondente à data para o tipo datetime e convertendo para o formato "Period" de frequência "15T" (a cada 15 minutos)
  # A biblioteca 'sktime' só consegue lidar com o campo 'Data' se estiver neste formato de dado
  # df['data_hora'] = pd.DatetimeIndex(df['data_hora']).to_period('15T')

  # A partir do objeto criado anteriormente, agora eu 'corto' entre TREINO e VALIDAÇÃO
  # Será 75% para TESTE e 25% para VALIDAÇÃO calculados à partir do início da série temporal
  # treino, validacao = temporal_train_test_split(df, train_size=0.75, anchor="start")
  # treino.name, validacao.name = 'Treino', 'Validação'

  # ForecastHorizon, o horizonte de tempo que pretende-se realizar a previsão.
  # Está em questão de meses porque os dados são mensais.
  # A ser utilizado com a biblioteca 'sktime'
  # fh = pd.PeriodIndex(data=validacao.index.values)

  # As colunas importantes para entrada no NeuralForecast/LSTM são mesmo "unique_id", "data_hora" e "vazao".
  # Contudo, se colocar mais coluna que conserva relação com o alvo ("vazao"), isso ajuda a aprimorar mais o resultado do modelo. => DICA IMPORTANTE
  # É com isso em mente que entra a coluna "chuva"
  feature_names = ['chuva', 'vazao']
  target_names = 'vazao'

  # O NeuralForecast/LSTM demanda uma coluna nomeada "unique_id"
  # A coluna serve como identificador ÚNICO da série temporal e pode ser qualquer identificador que separe
  # df['unique_id'] = 1

  # Ajustando as colunas "chuva" e "vazao" para tipo numérico
  df.chuva = pd.to_numeric(df.chuva, errors='coerce')
  df.vazao = pd.to_numeric(df.vazao, errors='coerce')

  # Botando o campo "data_hora" pra índice do DataFrame
  df.set_index('data_hora', inplace=True)

  # Agregando os dados por dia e aplicando a média para "vazao" e somatório para "chuva"
  # Além de reduzir o dataset isso também é importante caso tenha muitos dados faltantes (o que é bastante comum)
  df_media_diaria = df.resample('D').agg({'chuva': np.sum, 'vazao': np.mean})

  dataset =  {
    'task'              : 'Análise de Série Temporal de estação telemétrica',
    'name'              : str('Dados telemétricos - estação %s' % codigoEstacao),
    'reference'         : "...",
    'feature_names'     : feature_names,
    'target_names'      : target_names,
    'n_samples'         : len(df),
    'n_features'        : len(feature_names),
    'full_serie'        : df,
    'agg_by_day'        : df_media_diaria,
    'description'       : "Dados de telemetria disponibilizados pela Agência Nacional de Águas e Saneamento Básico do Brasil - ANA",
    'raw_data'          : link,
    'station_code'      : codigoEstacao,
    'station_type'      : tipoEstacao,
    }

  return (dataset)

# Fazendo a carga dos dados a serem analisados
ds = read_telemetrica_chuva_vazao(codigoEstacao='56994500',
                                  tipoEstacao='Telemétrica',
                                  chave="1hl9KK8cmu3DyRaFHwa3E73KdH5nbDhByW-IySNY5oX0")

ds['agg_by_day'].chuva.plot(title="Chuva agregado por dia (somatório)", color='b')

ds['agg_by_day'].vazao.plot(title="Vazão agregada por dia (média)", color='g')

"""# Pré-processamento dos dados"""

ds['agg_by_day'].dtypes

# Olhando se tem dados faltantes (0 ou NaN) e contando.

ds['agg_by_day'].isna().sum()

# Para este dataset, vale à pena mesmo pegar dados a partir de 2021. Tem um salto de anos muito longo, de 2015 para 2021.
# E como para 2015 os dados de chuva estão todos "NaN", seriam excluídos de qualquer forma mesmo.
# Contudo, deixando ".year >= 2021" apenas para compreensão da linha de raciocínio seguida por mim (Welson)

len(ds['agg_by_day'][ds['agg_by_day'].index.year >= 2021]), ds['agg_by_day'][ds['agg_by_day'].index.year >= 2021].isna().sum()

df_media_diaria_2021 = ds['agg_by_day'][ds['agg_by_day'].index.year >= 2021]
len(df_media_diaria_2021)

# df_media_diaria.dropna(inplace=True)
# df_media_diaria.shape

df_media_diaria_2021.isna().sum()

#É possível ver que existem dados de vazão NaN. Estes dados serão cortados
df_media_diaria_2021.dropna(inplace=True)

df_media_diaria_2021.info()

"""# Análise exploratória dos dados"""

# Especificando a frequência como "diária"

df_media_diaria_2021.asfreq('D')

df_media_diaria_2021.index.freq
df_media_diaria_2021.index

# Plotando a série de chuva
df_media_diaria_2021.chuva.plot(title='Chuva', color='b', xlabel='Meses', ylabel='Precipitação (mm)')

# Plotando a série de vazão
df_media_diaria_2021.vazao.plot(title='Vazão', color='g', xlabel='Meses', ylabel='Vazão ($m^3/s$)')

# Decompondo cada uma das séries analisadas
from statsmodels.tsa.seasonal import seasonal_decompose

r_chuva = seasonal_decompose(df_media_diaria_2021.chuva, period=120, model="add")
r_chuva.plot();

r_vazao = seasonal_decompose(df_media_diaria_2021.vazao, period=360, model="mult")
r_vazao.plot();

# Plotando o ACF para chuva e vazão pra detectar a sazonalidade
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

fig, axs = plt.subplots(nrows=2, ncols=1, sharex=True)
plot_acf(df_media_diaria_2021.chuva, ax=axs[0], lags=100, alpha=0.05)
axs[0].set_title('ACF "chuva"')
plot_acf(df_media_diaria_2021.vazao, ax=axs[1], lags=100, alpha=0.05)
axs[1].set_title('ACF "vazao"')

# Hidrograma da estação
sns.set_style("whitegrid")

fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

sns.lineplot(x=df_media_diaria_2021.index, y="chuva", data=df_media_diaria_2021, color="blue", ax=ax2)
ax2.fill_between(df_media_diaria_2021.index, 0, df_media_diaria_2021["chuva"], color='blue', alpha = 0.4)
ax2.set(ylim=(0, 40))
ax2.set_ylabel("Chuva [mm]")
ax2.invert_yaxis()

sns.lineplot(x=df_media_diaria_2021.index, y="vazao", data=df_media_diaria_2021, color="red", ax=ax1)
ax1.set(ylim=(0, 8000))
ax1.set_title('Hidrograma da estação')
ax1.set_ylabel("Vazão [$m^3$/s]")
ax1.set_xlabel("Meses")

# Precisa equalizar as unidades de medida. Converter m^3 para mm
# Fator de conversão de m^3 para mm^3 -> [mm^3 = m^3 x 1.000.000.000] (1 bilhão)
df_media_diaria_2021.vazao = df_media_diaria_2021.vazao * 1000000000
df_media_diaria_2021

# Vou tabular a série temporal pegando 3 lags pra isso.
# Desta forma, considero que os "t-3" dias anteriores medidos influeciam a medição no tempo "t" e também a chuva prevista para o dia "t".
tab_data = sliding_window(df_media_diaria_2021.values, n_lags=3, n_targets=1)

# Alterando os nomes das colunas para ficar mais legível.
tab_data.columns=['X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'vazao']
# tab_data

# Separo em dois DataFrames. DataFrame de input (X) e DataFrame de output (y)
X = tab_data[['X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7']]
y = tab_data[['vazao']]

# Resetando os índices e removendo a coluna de índices antiga
X.reset_index(drop=True, inplace=True), y.reset_index(drop=True, inplace=True)

# Dividindo os dados entre "treino" e "validação".
X_training, X_validation = split_train_test(X, 0.8)
y_training, y_validation = split_train_test(y, 0.8)

X_training.shape, X_validation.shape, y_training.shape, y_validation.shape

"""Um passo importante a se fazer com valores muito grandes é fazer a normalização.
Como a vazão foi convertida de $m^3$ para $mm^3$, isso PRECISA ser feito sob risco da rede se perder.
"""

# Normalizando os valores pois tem valores EXTREMAMENTE distantes.

scaler = MinMaxScaler(feature_range=(0, 1))

X_training_scaled = scaler.fit_transform(X_training) # Apenas os dados de treinamento passam por fit
X_validation_scaled = scaler.transform(X_validation)

y_training_scaled = scaler.fit_transform(y_training) # Apenas os dados de treinamento passam por fit
y_validation_scaled = scaler.transform(y_validation)

# inverse transform and print
# inversed = scaler.inverse_transform(normalized)
# print(inversed)

# X_training_scaled.shape, X_validation_scaled.shape, y_training_scaled.shape, y_validation_scaled.shape

# Usando o sweetviz para avaliar
# import sweetviz as sv
# analyze_report = sv.analyze(tab_data)
# analyze_report.show_html('analyze.html', open_browser=False)

# Apresentando os resultados
# import IPython
# IPython.display.HTML('analyze.html')

"""Ter separado em intervalo de 3 lags parece uma boa.
As associações percebidas pelo SweetViz mostram que uma chuva num dia 't' influencia menos as chuvas 't+2' e 't+3' dias depois.

### Teste de Estacionariedade
"""

from statsmodels.tsa.stattools import adfuller

# result = adfuller(df_media_diaria_2021.vazao.diff(1).dropna())
result = adfuller(df_media_diaria_2021.vazao)
print(f'Estatística ADF: {result[0]}')
print(f'p-value: {result[1]}')
print('Valores Críticos:')
for key, value in result[4].items():
   print(f'\t{key}: {value}')

"""A série de vazões original é uma série não-estacionária. Apresenta tendência ou sazonalidade, ou ambos."""

result = adfuller(df_media_diaria_2021.chuva)
print(f'Estatística ADF: {result[0]}')
print(f'p-value: {result[1]}')
print('Valores Críticos:')
for key, value in result[4].items():
   print(f'\t{key}: {value}')

"""Já a série de chuvas é estacionária.

# LSTM

Usando a implementação da biblioteca **KERAS**
"""

# Variável para armazenar todos os cálculos de desempenho dos modelos empregados
desempenho = {}

"""## Preparando os dados"""

# X de treino
X_lstm = X_training_scaled.reshape(X_training_scaled.shape[0], X_training_scaled.shape[1], 1)
# X_lstm.shape

# y de treino
y_lstm = y_training_scaled.reshape(y_training_scaled.shape[0], y_training_scaled.shape[1], 1)
# y_lstm.shape

# X para pevisão
X_pred_lstm = X_validation_scaled.reshape(X_validation_scaled.shape[0], X_validation_scaled.shape[1], 1)
# X_pred_lstm.shape

"""## Vanilla LSTM"""

vanilla_lstm = Sequential()
vanilla_lstm.add(LSTM(128, activation='relu', input_shape=(X_lstm.shape[1], 1)))
vanilla_lstm.add(Dense(1))
vanilla_lstm.compile(optimizer='adam', loss='mse')

vanilla_lstm.fit(X_lstm, y_lstm, epochs=250, verbose=0)

vazao_pred = vanilla_lstm.predict(X_pred_lstm, verbose=0)

vazao_pred_df = pd.DataFrame(data=vazao_pred, columns=['pred'])
vazao_pred_df['observed'] = y_validation_scaled

# vazao_pred_df

plot_series(vazao_pred_df['observed'], vazao_pred_df['pred'],
            labels=["observado", "previsão"],
            x_label='',
            y_label='Vazão ($m^3/s$)',
            title='Vanilla LSTM')

desempenho['VanillaLSTM'] = {'MAPE': mean_absolute_percentage_error(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values),
                             'R2': r2_score(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values),
                             'MSE' : mean_squared_error(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values)}

"""## Bidirectional LSTM

A característica deste modelo é que ele guarda e passa informações para a célula adiante e retorna para células anteriores informações que foram ajustadas à frente, na rede. Isso potencializa o aprendizado do modelo.
"""

bidir_lstm = Sequential()
bidir_lstm.add(Bidirectional(LSTM(128, activation='relu', input_shape=(X_lstm.shape[1], 1))))
bidir_lstm.add(Dense(1))
bidir_lstm.compile(optimizer='adam', loss='mse')

bidir_lstm.fit(X_lstm, y_lstm, epochs=250, verbose=0)

vazao_pred = bidir_lstm.predict(X_pred_lstm, verbose=0)

vazao_pred_df = pd.DataFrame(data=vazao_pred, columns=['pred'])
vazao_pred_df['observed'] = y_validation_scaled

# vazao_pred_df

plot_series(vazao_pred_df['observed'], vazao_pred_df['pred'],
            labels=["observado", "previsão"],
            x_label='',
            y_label='Vazão ($m^3/s$)',
            title='Bidirectional LSTM')

desempenho['BidirectionalLSTM'] = {'MAPE': mean_absolute_percentage_error(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values),
                                   'R2': r2_score(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values),
                                   'MSE' : mean_squared_error(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values)}

"""## Stacked LSTM

A ideia aqui é criar um modelo com mais de uma camada oculta, visando o aprimoramento dos resultados.
"""

stkd_lstm = Sequential()
stkd_lstm.add(LSTM(128, activation='relu', return_sequences=True, input_shape=(X_lstm.shape[1], 1)))
stkd_lstm.add(LSTM(128, activation='relu'))
stkd_lstm.add(Dense(1))
stkd_lstm.compile(optimizer='adam', loss='mse')

stkd_lstm.fit(X_lstm, y_lstm, epochs=250, verbose=0)

vazao_pred = stkd_lstm.predict(X_pred_lstm, verbose=0)

vazao_pred_df = pd.DataFrame(data=vazao_pred, columns=['pred'])
vazao_pred_df['observed'] = y_validation_scaled

# vazao_pred_df

plot_series(vazao_pred_df['observed'], vazao_pred_df['pred'],
            labels=["observado", "previsão"],
            x_label='',
            y_label='Vazão ($m^3/s$)',
            title='Stacked LSTM')

desempenho['StackedLSTM'] = {'MAPE': mean_absolute_percentage_error(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values),
                             'R2': r2_score(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values),
                             'MSE' : mean_squared_error(vazao_pred_df['observed'].values, vazao_pred_df['pred'].values)}

# Faz o print da qualidade dos resultados de cada método empregado

# for _m1 in desempenho.items():
#   print(_m1[0] + "\n\t" + "MAPE:\t" + str(_m1[1]['MAPE'])
#                + "\n\t" + "R2:\t"   + str(_m1[1]['R2'])
#                + "\n\t" + "MSE:\t"  + str(_m1[1]['MSE']))

desempenho_df = pd.DataFrame(data=desempenho)
desempenho_df

# Executando os 3 modelos em lote

k = ''
# vazao_pred_batch = pd.DataFrame(data=y_validation_scaled, columns=['valid'])
vazao_pred_van, vazao_pred_bidir, vazao_pred_stkd = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
for i in range(0, 15):
  k = 'van_run_' + str(i+1)
  vanilla_lstm.fit(X_lstm, y_lstm, epochs=200, verbose=0)
  van_pred = pd.DataFrame(data=vanilla_lstm.predict(X_pred_lstm, verbose=0))
  vazao_pred_van[k] = van_pred

  k = 'bidir_run_' + str(i+1)
  bidir_lstm.fit(X_lstm, y_lstm, epochs=200, verbose=0)
  bidir_pred = pd.DataFrame(data=bidir_lstm.predict(X_pred_lstm, verbose=0))
  vazao_pred_bidir[k] = bidir_pred

  k = 'stkd_run_' + str(i+1)
  stkd_lstm.fit(X_lstm, y_lstm, epochs=200, verbose=0)
  stkd_pred = pd.DataFrame(data=stkd_lstm.predict(X_pred_lstm, verbose=0))
  vazao_pred_stkd[k] = stkd_pred

# vazao_pred_van
# vazao_pred_bidir
# vazao_pred_stkd

"""# Loading AirPassengers Data

The `core.NeuralForecast` class contains shared, `fit`, `predict` and other methods that take as inputs pandas DataFrames with columns `['unique_id', 'ds', 'y']`, where `unique_id` identifies individual time series from the dataset, `ds` is the date, and `y` is the target variable.

In this example dataset consists of a set of a single series, but you can easily fit your model to larger datasets in long format.
"""

Y_df = AirPassengersDF # Defined in neuralforecast.utils
Y_df.head()

""":::{.callout-important}
DataFrames must include all `['unique_id', 'ds', 'y']` columns.
Make sure `y` column does not have missing or non-numeric values.
:::
"""

import numpy as np
key='1OKeFE_6M553VnYzAQOTnkozp0ma5Rvg2fWR4qwYyC68'
link='https://docs.google.com/spreadsheet/ccc?key='+key+'&output=csv'
dataset = pd.read_csv(link, sep=',')
dataset['data'] = [i.split(' ')[0] for i in dataset['Data']]
dataset=dataset.groupby('data').agg(np.mean)
dataset['data'] = pd.to_datetime(dataset.index, format='%Y-%m-%d')
dataset = dataset.fillna(dataset.mean())
dataset.drop(['Nível', ], axis=1, inplace=True)
#dataset['Vazão'].plot()
df=dataset
df['datetime'] = df.index; df.index=df['datetime']
df['value']=df['Vazão']

df['datetime'] = pd.to_datetime(df['datetime'])

# Plot data
df.plot('datetime', 'value')
plt.show()

Y_df=pd.DataFrame()
Y_df['ds']=df['datetime']
Y_df['y']=df['Vazão']
Y_df['unique_id'] = 1.0
Y_df['y'].plot()
Y_df=Y_df[['unique_id', 'ds', 'y']]
Y_df

horizon = 720
n=len(Y_df)-horizon
Y_df_test = Y_df[-horizon::]
Y_df_train  = Y_df[:n]

n, horizon, len(Y_df_train), len(Y_df_test)

Y_df_train['y'].plot()
Y_df_test['y'].plot()

"""## 3. Model Training

### Fit the models

Using the `NeuralForecast.fit` method you can train a set of models to your dataset. You can define the forecasting `horizon` (12 in this example), and modify the hyperparameters of the model. For example, for the `LSTM` we changed the default hidden size for both encoder and decoders.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# # Try different hyperparmeters to improve accuracy.
# models = [
#           LSTM(h=horizon,                    # Forecast horizon
#                max_steps=500,                # Number of steps to train
#                scaler_type='standard',       # Type of scaler to normalize data
#                encoder_hidden_size=32,       # Defines the size of the hidden state of the LSTM
#                decoder_hidden_size=32,),     # Defines the number of hidden units of each layer of the MLP decoder
# 
#           NHITS(h=horizon,                   # Forecast horizon
#                 input_size=2 * horizon,      # Length of input sequence
#                 max_steps=100,               # Number of steps to train
#                 n_freq_downsample=[2, 1, 1]) # Downsampling factors for each stack output
# 
#           #AutoRNN(h=horizon,
#           #        config=dict(max_steps=2, val_check_steps=1, input_size=-1, encoder_hidden_size=8),
#           #        num_samples=1, cpus=1),
#           ]
# nf = NeuralForecast(models=models, freq='D')
# nf.fit(df=Y_df_train)

""":::{.callout-tip}
The performance of Deep Learning models can be very sensitive to the choice of hyperparameters. Tuning the correct hyperparameters is an important step to obtain the best forecasts. The `Auto` version of these models, `AutoLSTM` and `AutoNHITS`, already perform hyperparameter selection automatically.
:::

### Predict using the fitted models

Using the `NeuralForecast.predict` method you can obtain the `h` forecasts after the training data `Y_df`.
"""

Y_hat_df = nf.predict()
Y_hat_df

"""The `NeuralForecast.predict` method returns a DataFrame with the forecasts for each `unique_id`, `ds`, and model."""

Y_hat_df = Y_hat_df.reset_index()
Y_hat_df['y']=Y_df_test['y'].values
Y_hat_df#.head()

"""## 4. Plot Predictions

Finally, we plot the forecasts of both models againts the real values.
"""

fig, ax = plt.subplots(1, 1, figsize = (20, 7))
plot_df = pd.concat([Y_df_test, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes
plot_df[['y',
         'LSTM',
         'NHITS',
         #'AutoRNN',
         ]].plot(ax=ax, linewidth=2)

#ax.set_title('AirPassengers Forecast', fontsize=22)
#ax.set_ylabel('Monthly Passengers', fontsize=20)
#ax.set_xlabel('Timestamp [t]', fontsize=20)
#ax.legend(prop={'size': 15})
#ax.grid()

""":::{.callout-tip}
For this guide we are using a simple `LSTM` model. More recent models, such as `RNN`, `GRU`, and `DilatedRNN` achieve better accuracy than `LSTM` in most settings. The full list of available models is available [here](https://nixtla.github.io/neuralforecast/models.html).
:::
"""

fig, ax = plt.subplots(1, 1, figsize = (20, 7))

Y_hat_df.index=Y_df_test.index
Y_hat_df[['y',
         'LSTM',
         'NHITS',
         #'AutoRNN',
         ]].plot(ax=ax)

"""## References
- [Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2020). "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting". International Conference on Learning Representations.](https://arxiv.org/abs/1905.10437)<br>
- [Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski (2021). NHITS: Neural Hierarchical Interpolation for Time Series Forecasting. Accepted at AAAI 2023.](https://arxiv.org/abs/2201.12886)
"""