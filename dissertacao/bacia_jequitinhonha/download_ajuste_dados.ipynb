{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  warnings,                   \\\n",
    "        calendar,                   \\\n",
    "        pandas as pd,               \\\n",
    "        numpy as np,                \\\n",
    "        requests as rt,             \\\n",
    "        hydrobr as hbr,             \\\n",
    "        xml.etree.ElementTree as ET\n",
    "from typing import List\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "# Desativar as mensagens de 'warning' que ficam poluindo o output de alguns trechos de código.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dados(file_name : str,\n",
    "                   separator : str = \"\\t\",\n",
    "                   adjust : bool = True,\n",
    "                   date_column : str = \"ds\"\n",
    "                   ) -> pd.DataFrame:\n",
    "    \n",
    "    df = pd.read_csv(file_name, sep=separator, index_col=date_column, header=0, parse_dates=[date_column])\n",
    "\n",
    "    if adjust:\n",
    "        df = df.resample('D').first() # deixando a série contínua numa base diária\n",
    "\n",
    "    # Deixando ajustado para usar com as libs Nixtla\n",
    "    df['unique_id'] = 1\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df\n",
    "# ============================================================================================ #\n",
    "def get_telemetrica(codEstacao : str,\n",
    "                    dataInicio : str,\n",
    "                    dataFim : str,\n",
    "                    save : bool = False) -> pd.DataFrame:\n",
    "    # 1. Fazer a requisião ao servidor e pegar a árvore e a raiz dos dados \n",
    "    params = {'codEstacao':codEstacao, 'dataInicio':dataInicio, 'dataFim':dataFim}\n",
    "    server = 'http://telemetriaws1.ana.gov.br/ServiceANA.asmx/DadosHidrometeorologicos'\n",
    "    response = rt.get(server, params)\n",
    "    tree = ET.ElementTree(ET.fromstring(response.content))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 2. Iteração dentro dos elementos do XML procurando os dados que são disponibilizados para a estação\n",
    "    list_vazao = []\n",
    "    list_data = []\n",
    "    list_cota = []\n",
    "    list_chuva = []\n",
    "\n",
    "    for i in root.iter('DadosHidrometereologicos'):\n",
    "\n",
    "        data = i.find('DataHora').text\n",
    "        try:\n",
    "            vazao = float(i.find('Vazao').text)\n",
    "        except TypeError:\n",
    "            vazao = i.find('Vazao').text\n",
    "\n",
    "        try:\n",
    "            cota = float(i.find('Nivel').text)\n",
    "        except TypeError:\n",
    "            cota = i.find('Nivel').text\n",
    "\n",
    "        try:\n",
    "            chuva = float(i.find('Chuva').text)\n",
    "        except TypeError:\n",
    "            chuva = i.find('Chuva').text\n",
    "\n",
    "        list_vazao.append(vazao)\n",
    "        list_data.append(data)\n",
    "        list_cota.append(cota)\n",
    "        list_chuva.append(chuva)\n",
    "\n",
    "    df = pd.DataFrame([list_data, list_cota, list_chuva, list_vazao]).transpose()\n",
    "    df.columns = ['Data', 'Cota', 'Chuva', 'Vazao']\n",
    "    df = df.sort_values(by='Data')\n",
    "    df = df.set_index('Data')\n",
    "    \n",
    "    if save == True:\n",
    "        df.to_excel(codEstacao+'_dados_tele.xlsx')\n",
    "    \n",
    "    return df\n",
    "# ============================================================================================ #\n",
    "def get_convencional(codEstacao : str,\n",
    "                     dataInicio : str,\n",
    "                     dataFim : str,\n",
    "                     tipoDados : int,\n",
    "                     nivelConsistencia : int,\n",
    "                     save : bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Série Histórica estação - HIDRO.\n",
    "        codEstacao : Código Plu ou Flu\n",
    "        dataInicio : <YYYY-mm-dd>\n",
    "        dataFim : Caso não preenchido, trará até o último dado mais recente armazenado\n",
    "        tipoDados : 1-Cotas, 2-Chuvas ou 3-Vazões\n",
    "        nivelConsistencia : 1-Bruto ou 2-Consistido\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Fazer a requisião ao servidor e pegar a árvore e a raiz dos dados \n",
    "    params = {'codEstacao':codEstacao, 'dataInicio':dataInicio, 'dataFim':dataFim,\n",
    "              'tipoDados':tipoDados, 'nivelConsistencia':nivelConsistencia}\n",
    "    \n",
    "    server = 'http://telemetriaws1.ana.gov.br/ServiceANA.asmx/HidroSerieHistorica'\n",
    "    response = rt.get(server, params)\n",
    "    tree = ET.ElementTree(ET.fromstring(response.content))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # 2. Iteração dentro dos elementos do XML procurando os dados que são disponibilizados para a estação\n",
    "    list_data = []\n",
    "    list_consistenciaF = []\n",
    "    list_month_dates = []\n",
    "\n",
    "    for i in root.iter('SerieHistorica'):\n",
    "\n",
    "        consistencia = i.find('NivelConsistencia').text\n",
    "        date = i.find('DataHora').text\n",
    "        date = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        last_day = calendar.monthrange(date.year, date.month)[1]\n",
    "        month_dates = [date + timedelta(days=i) for i in range(last_day)]\n",
    "        content = []\n",
    "        list_consistencia = []\n",
    "\n",
    "        for day in range(last_day):\n",
    "            if tipoDados == 1:\n",
    "                value = f'Cota{day+1:02d}'\n",
    "            if tipoDados == 2:\n",
    "                value = f'Chuva{day+1:02d}'\n",
    "            if tipoDados == 3:\n",
    "                value = f'Vazao{day+1:02d}'\n",
    "            \n",
    "            try:\n",
    "                content.append(float(i.find(value).text))\n",
    "                list_consistencia.append(int(consistencia))\n",
    "            except TypeError:\n",
    "                content.append(i.find(value).text)\n",
    "                list_consistencia.append(int(consistencia))\n",
    "            except AttributeError:\n",
    "                content.append(None)\n",
    "                list_consistencia.append(int(consistencia))\n",
    "        \n",
    "        list_data += content\n",
    "        list_consistenciaF += list_consistencia\n",
    "        list_month_dates += month_dates\n",
    "    df = pd.DataFrame([list_month_dates, list_consistenciaF, list_data]).transpose()\n",
    "\n",
    "    if tipoDados == 1:\n",
    "        df.columns = ['Data','Consistencia','Cota']\n",
    "    elif tipoDados == 2:\n",
    "        df.columns = ['Data','Consistencia','Chuva']\n",
    "    else: # Vazão\n",
    "        df.columns = ['Data','Consistencia','Vazao']\n",
    "    \n",
    "    df = df.sort_values(by='Data')\n",
    "    df = df.set_index('Data')\n",
    "\n",
    "    if save == True:\n",
    "        df.to_excel(codEstacao + '_dados_conv.xlsx')\n",
    "    \n",
    "    return df\n",
    "# ============================================================================================ #\n",
    "def gerar_dados_tele(estacao_principal : str,\n",
    "                    outras_estacoes : List[str],\n",
    "                    nome_arq : str,\n",
    "                    dt_inicio : str,\n",
    "                    dt_fim : str,\n",
    "                    salvar : bool = False) -> None:\n",
    "    \"\"\"\n",
    "            Este método vai pegar o código da 'estacao_principal' (que o usuário já sabe previamente que é uma telemétrica), baixar os dados da estação\n",
    "        e concatenar (outer join) com os dados das outras estações telemétricas. Neste método já será realizada a conversão dos dados de 'object' para\n",
    "        os tipos de acordo, ou seja, 'float' para os campos numéricos e 'datetime' para os campos de datahora.\n",
    "            Como o desejo do trabalho é lidar com dados diários, já aproveita pra fazer a agregação dos dados desta maneira também.\n",
    "            Após tudo isso, salva num arquivo xlsx para usos posteriores.\n",
    "\n",
    "        Parâmetros:\n",
    "            estacao_principal : str,\n",
    "            outras_estacoes : List[str],\n",
    "            nome_arq : str,\n",
    "            dt_inicio : str = 'YYYY-mm-dd',\n",
    "            dt_fim : str = 'YYYY-mm-dd',\n",
    "            salvar : bool = True|False\n",
    "    \"\"\"\n",
    "\n",
    "    df_result = get_telemetrica(codEstacao=estacao_principal, dataInicio=dt_inicio, dataFim=dt_fim)\n",
    "\n",
    "    df_result.index = pd.to_datetime(df_result.index)\n",
    "    df_result.Cota = pd.to_numeric(df_result.Cota, errors='coerce')\n",
    "    df_result.Chuva = pd.to_numeric(df_result.Chuva, errors='coerce')\n",
    "    df_result.Vazao = pd.to_numeric(df_result.Vazao, errors='coerce')\n",
    "\n",
    "    df_result = df_result.resample('D').agg({'Cota': 'mean', 'Chuva': 'sum', 'Vazao': 'mean'})\n",
    "\n",
    "    df_result.columns = ['t_ct_'+str(estacao_principal), 't_cv_'+str(estacao_principal), 't_vz_'+str(estacao_principal)]\n",
    "\n",
    "    # Agora que já tenho os dados da estação que considero principal na análise (target)\n",
    "    #   vou agregar com os dados das demais estações\n",
    "\n",
    "    if outras_estacoes is not None:\n",
    "        for e in outras_estacoes:\n",
    "            df_temp = get_telemetrica(codEstacao=e, dataInicio=dt_inicio, dataFim=dt_fim)\n",
    "\n",
    "            # Convertendo os dados\n",
    "            df_temp.index = pd.to_datetime(df_temp.index)\n",
    "            df_temp.Cota = pd.to_numeric(df_temp.Cota, errors='coerce')\n",
    "            df_temp.Chuva = pd.to_numeric(df_temp.Chuva, errors='coerce')\n",
    "            df_temp.Vazao = pd.to_numeric(df_temp.Vazao, errors='coerce')\n",
    "\n",
    "            # Para as telemétricas já agrego aqui mesmo\n",
    "            df_temp = df_temp.resample('D').agg({'Cota': 'mean', 'Chuva': 'sum', 'Vazao': 'mean'})\n",
    "\n",
    "            # Ajeito os nomes das colunas pra conter de qual estacao os dado veio\n",
    "            df_temp.columns = ['t_ct_'+e, 't_cv_'+e, 't_vz_'+e]\n",
    "\n",
    "            df_result = pd.concat([df_result, df_temp], axis=1)\n",
    "\n",
    "    if salvar:\n",
    "        df_result.to_excel(nome_arq+'_dados_tele.xlsx')\n",
    "# ============================================================================================ #\n",
    "def gerar_dados_conv(estacao_principal : str,\n",
    "                    outras_estacoes : List[str],\n",
    "                    nome_arq : str,\n",
    "                    dt_inicio : str,\n",
    "                    dt_fim : str,\n",
    "                    tp_dados : int,\n",
    "                    nvl_consistencia : str,\n",
    "                    drop_consistencia : bool = True, # Remover a coluna \"NivelConsistência\". Ela será irrelevante, até segunda ordem.\n",
    "                    salvar : bool = False) -> None:\n",
    "    \"\"\"\n",
    "            Este método vai pegar o código da 'estacao_principal' (que o usuário já sabe previamente que é uma convencional), baixar os dados da estação\n",
    "        e concatenar (outer join) com os dados das outras estações convencionais. Neste método já será realizada a conversão dos dados de 'object' para\n",
    "        os tipos de acordo, ou seja, 'float' para os campos numéricos e 'datetime' para os campos de datahora.\n",
    "            Como o desejo do trabalho é lidar com dados diários, já aproveita pra fazer a agregação dos dados desta maneira também.\n",
    "            Após tudo isso, salva num arquivo xlsx para usos posteriores.\n",
    "\n",
    "        Parâmetros:\n",
    "            estacao_principal : str,\n",
    "            outras_estacoes : List[str],\n",
    "            nome_arq : str,\n",
    "            dt_inicio : str = 'YYYY-mm-dd',\n",
    "            dt_fim : str = 'YYYY-mm-dd',\n",
    "            tp_dados : int (1-cota | 2-chuva | 3-vazao),\n",
    "            nvl_consistencia : int (1-bruto | 2-consistido),\n",
    "            drop_consistencia : bool = True, (Remover a coluna \"NivelConsistência\". Ela será irrelevante, até segunda ordem)\n",
    "            salvar : bool = False\n",
    "    \"\"\"\n",
    "\n",
    "    df_result = get_convencional(codEstacao=estacao_principal, dataInicio=dt_inicio, dataFim=dt_fim, tipoDados=tp_dados, nivelConsistencia=nvl_consistencia)\n",
    "\n",
    "    df_result.index = pd.to_datetime(df_result.index)\n",
    "\n",
    "    if drop_consistencia:\n",
    "        df_result.drop(columns=['Consistencia'], inplace=True)\n",
    "\n",
    "    if tp_dados == 1:\n",
    "        df_result.Cota = pd.to_numeric(df_result.Cota, errors='coerce')\n",
    "        df_result = df_result.resample('D').agg({'Cota': 'mean'})\n",
    "        df_result.columns = ['c_ct_'+str(estacao_principal)]\n",
    "    elif tp_dados == 2:\n",
    "        df_result.Chuva = pd.to_numeric(df_result.Chuva, errors='coerce')\n",
    "        df_result = df_result.resample('D').agg({'Chuva': 'sum'})\n",
    "        df_result.columns = ['c_cv_'+str(estacao_principal)]\n",
    "    else: # Vazão\n",
    "        df_result.Vazao = pd.to_numeric(df_result.Vazao, errors='coerce')\n",
    "        df_result = df_result.resample('D').agg({'Vazao': 'mean'})\n",
    "        df_result.columns = ['c_vz_'+str(estacao_principal)]\n",
    "\n",
    "    # Agora que já tenho os dados da estação que considero principal na análise (target)\n",
    "    #   vou agregar com os dados das demais estações\n",
    "\n",
    "    for e in outras_estacoes:\n",
    "        df_temp = get_convencional(codEstacao=e, dataInicio=dt_inicio, dataFim=dt_fim, tipoDados=tp_dados, nivelConsistencia=nvl_consistencia)\n",
    "\n",
    "        # Convertendo os dados\n",
    "        df_temp.index = pd.to_datetime(df_temp.index)\n",
    "\n",
    "        if drop_consistencia:\n",
    "            df_temp.drop(columns=['Consistencia'], inplace=True)\n",
    "\n",
    "        if tp_dados == 1:\n",
    "            df_temp.Cota = pd.to_numeric(df_temp.Cota, errors='coerce')\n",
    "            df_temp = df_temp.resample('D').agg({'Cota': 'mean'})\n",
    "            df_temp.columns = ['c_ct_'+str(e)]\n",
    "        elif tp_dados == 2:\n",
    "            df_temp.Chuva = pd.to_numeric(df_temp.Chuva, errors='coerce')\n",
    "            df_temp = df_temp.resample('D').agg({'Chuva': 'sum'})\n",
    "            df_temp.columns = ['c_cv_'+str(e)]\n",
    "        else: # Vazão\n",
    "            df_temp.Vazao = pd.to_numeric(df_temp.Vazao, errors='coerce')\n",
    "            df_temp = df_temp.resample('D').agg({'Vazao': 'mean'})\n",
    "            df_temp.columns = ['c_vz_'+str(e)]\n",
    "\n",
    "        df_result = pd.concat([df_result, df_temp], axis=1)\n",
    "\n",
    "    if salvar:\n",
    "        if tp_dados == 1:\n",
    "            df_result.to_excel(nome_arq + '_dados_cota_conv.xlsx')\n",
    "        elif tp_dados == 2:\n",
    "            df_result.to_excel(nome_arq + '_dados_chuva_conv.xlsx')\n",
    "        else:\n",
    "            df_result.to_excel(nome_arq + '_dados_vazao_conv.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Médio Rio Jequitinhonha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baixando os dados das estações que serão utilizadas no trabalho\n",
    "# # As estações foram selecionadas a partir do sistema Data Rhama\n",
    "# # Aqui eu baixo os dados e salvo localmente\n",
    "# # >>>>>>>>>>>>> SÓ PRECISA FAZER ISSO UMA VEZ, POR ISSO O CÓDIGO FICA COMENTADO DEPOIS DE RODAR!!!! <<<<<<<<<<<<<\n",
    "\n",
    "# estacao_principal = '56920000'\n",
    "# outras_estacoes = ['01941018', '56846900', '56846890', '01841011', '56850000', '56846200', '56895000', '01841020', '01841029']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemétricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aplicando a lib HydroBR eu desejo saber se as estações em questão são do tipo convencional ou telemétrica\n",
    "# # O código não exclui o fato, eventual, de uma dada estação ser convencional E telemétrica, como é o caso aqui\n",
    "\n",
    "# lista_estacoes = hbr.get_data.ANA.list_telemetric() # Vendo primeiro se tem telemétrica\n",
    "# lista_estacoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Averiguando se as estações que tenho em mãos estão presentes neste conjunto de estações telemétricas\n",
    "\n",
    "# print(\n",
    "#     \"Estação {e} -> {p}\".format(\n",
    "#         e=estacao_principal,\n",
    "#         p=(lista_estacoes['Code'] == estacao_principal).any()\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # A estação principal em questão tem dados telemétricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verificando as outras estações\n",
    "\n",
    "# for e in outras_estacoes:\n",
    "#     print(\n",
    "#         \"Estação {e} -> {p}\".format(\n",
    "#             e=e,\n",
    "#             p=(lista_estacoes['Code'] == e).any()\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # Estas estações também têm dados telemétricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gerando um arquivo Excel com os dados das estações telemétricas\n",
    "\n",
    "# estacoes_tele = [\"56850000\", \"56846200\", \"56895000\", \"01841029\"]\n",
    "# gerar_dados_tele(\n",
    "#     estacao_principal=estacao_principal,\n",
    "#     outras_estacoes=estacoes_tele,\n",
    "#     nome_arq=\"medio_rio_doce\",\n",
    "#     dt_inicio='2013-01-01',\n",
    "#     dt_fim='2023-12-31',\n",
    "#     salvar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convencionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cota/Vazão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aplicando a lib HydroBR eu desejo saber se as estações em questão são do tipo convencional ou telemétrica\n",
    "# # O código não exclui o fato, eventual, de uma dada estação ser convencional E telemétrica, como é o caso aqui\n",
    "\n",
    "# lista_estacoes = hbr.get_data.ANA.list_flow( # Verificando se tem estações de cota/vazão primeiro\n",
    "#     state='MINAS GERAIS',\n",
    "#     source='ANA'\n",
    "# )\n",
    "\n",
    "# lista_estacoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Averiguando se as estações que tenho em mãos estão presentes neste conjunto de estações convencionais de cota/vazão\n",
    "\n",
    "# print(\n",
    "#     \"Estação {e} -> {p}\".format(\n",
    "#         e=estacao_principal,\n",
    "#         p=(lista_estacoes['Code'] == estacao_principal).any()\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # A estação principal tem dados convencionais de cota/vazão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verificando as outras estações\n",
    "\n",
    "# for e in outras_estacoes:\n",
    "#     print(\n",
    "#         \"Estação {e} -> {p}\".format(\n",
    "#             e=e,\n",
    "#             p=(lista_estacoes['Code'] == e).any()\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # Estas estações também têm dados convencionais de cota/vazão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gerando um arquivo Excel com os dados das estações convencionais\n",
    "\n",
    "# estacoes_conv = [\"56846900\", \"56846890\", \"56850000\", \"56846200\", \"56895000\"]\n",
    "# gerar_dados_conv(\n",
    "#     estacao_principal=estacao_principal,\n",
    "#     outras_estacoes=estacoes_conv,\n",
    "#     nome_arq=\"medio_rio_doce\",\n",
    "#     dt_inicio='2013-01-01',\n",
    "#     dt_fim='2023-12-31',\n",
    "#     tp_dados=1, # Cota\n",
    "#     nvl_consistencia='2', # dados consistidos\n",
    "#     salvar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gerando um arquivo Excel com os dados das estações convencionais\n",
    "\n",
    "# gerar_dados_conv(\n",
    "#     estacao_principal=estacao_principal,\n",
    "#     outras_estacoes=estacoes_conv,\n",
    "#     nome_arq=\"medio_rio_doce\",\n",
    "#     dt_inicio='2013-01-01',\n",
    "#     dt_fim='2023-12-31',\n",
    "#     tp_dados=3, # Vazão\n",
    "#     nvl_consistencia='2', # dados consistidos\n",
    "#     salvar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_estacoes = hbr.get_data.ANA.list_prec(\n",
    "#     state='MINAS GERAIS',\n",
    "#     source='ANA'\n",
    "# )\n",
    "\n",
    "# lista_estacoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"Estação {e} -> {p}\".format(\n",
    "#         e=estacao_principal,\n",
    "#         p=(lista_estacoes['Code'] == estacao_principal).any()\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # A estação principal NÃO tem dados convencionais de chuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verificando as outras estações\n",
    "\n",
    "# for e in outras_estacoes:\n",
    "#     print(\n",
    "#         \"Estação {e} -> {p}\".format(\n",
    "#             e=e,\n",
    "#             p=(lista_estacoes['Code'] == e).any()\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # Estas estações também NÃO têm dados convencionais de chuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gerando um arquivo Excel com os dados das estações convencionais\n",
    "\n",
    "# estacoes_conv = [\"01941018\", \"01841011\", \"01841020\", \"01841029\"]\n",
    "# gerar_dados_conv(\n",
    "#     estacao_principal=estacao_principal,\n",
    "#     outras_estacoes=estacoes_conv,\n",
    "#     nome_arq=\"medio_rio_doce\",\n",
    "#     dt_inicio='2013-01-01',\n",
    "#     dt_fim='2023-12-31',\n",
    "#     tp_dados=2, # Chuva\n",
    "#     nvl_consistencia='2', # dados consistidos\n",
    "#     salvar=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Como estou em dúvida com o arquivo de cotas, deixei apenas estes dois arquivos pra trabalhar.\n",
    "\n",
    "# arquivos = ['medio_rio_doce_dados_tele.xlsx', 'medio_rio_doce_dados_vazao_conv.xlsx', 'medio_rio_doce_dados_chuva_conv.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vou fazer a carga primeiro dos dados telemétricos, porque é onde tem mais informação de uma única vez.\n",
    "# # Depois concateno os outros arquivos. Mas a ordem tanto faz aqui, só estipulei assim porque acho melhor\n",
    "\n",
    "# df = pd.read_excel(arquivos[0], sheet_name=0, index_col=0, header=0, parse_dates=['Data'])\n",
    "\n",
    "# for a in range(1, len(arquivos)):\n",
    "#     df_temp = pd.read_excel(arquivos[a], sheet_name=0, index_col=0, header=0, parse_dates=['Data'])\n",
    "#     df = pd.concat([df, df_temp], axis=1)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vou remover as colunas das cotas\n",
    "\n",
    "# df = df.drop(columns=['t_ct_56920000', 't_ct_56850000', 't_ct_56846200', 't_ct_56895000', 't_ct_01841029'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fazendo o merge das colunas de vazão que são correspondentes à mesma estação\n",
    "# # Acontece que existem gaps entre os dados, o que é estranho, porque a estação telemétrica tem dados que a convencional não tem e vice-versa.\n",
    "# # Vou contar qual coluna tem mais dados e depois executar um 'fillna'\n",
    "\n",
    "# colunas_esquerda = ['t_vz_56920000', 't_vz_56850000', 't_vz_56846200', 't_vz_56895000']\n",
    "# colunas_direita = ['c_vz_56920000', 'c_vz_56850000', 'c_vz_56846200', 'c_vz_56895000']\n",
    "\n",
    "# for i, j in zip(colunas_esquerda, colunas_direita):\n",
    "#     print(i, j, df[i].isna().sum(), df[j].isna().sum())\n",
    "\n",
    "# # A coluna que tiver menos buracos será preenchida com os dados da coluna que tem mais dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preenchendo a coluna que tem menos dados faltantes com a outra correspondente\n",
    "# # Fazer isso para cada coluna, contudo, tem colunas que tem dados faltando demais. Neste caso, darei drop nelas inteiramente.\n",
    "# df['c_vz_56920000'].fillna(df['t_vz_56920000'], inplace=True)\n",
    "# df['c_vz_56850000'].fillna(df['t_vz_56850000'], inplace=True)\n",
    "# df['t_vz_56846200'].fillna(df['c_vz_56846200'], inplace=True)\n",
    "# df['t_vz_56895000'].fillna(df['c_vz_56895000'], inplace=True)\n",
    "\n",
    "# df['c_vz_56920000'].isna().sum(), df['c_vz_56850000'].isna().sum(), df['t_vz_56846200'].isna().sum(), df['t_vz_56895000'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uma vez que as colunas estejam ajustadas, eu dropo as que não vou precisar mais\n",
    "# df = df.drop(columns=['t_vz_56920000', 't_vz_56850000', 'c_vz_56846200', 'c_vz_56895000', 't_vz_56895000'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verificando a quantidade de valores \"NaN\" em cada coluna\n",
    "\n",
    "# print(\"Quantidade de NaN por coluna\")\n",
    "# print(df.isna().sum())\n",
    "\n",
    "# print(\"Percentual de NaN por coluna\")\n",
    "# for c in np.asarray(df.columns):\n",
    "#     print(c, (df[c].isna().sum()/len(df))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removendo as colunas porque elas têm MUITOS  dados faltantes.\n",
    "\n",
    "# df = df.drop(columns=['t_cv_01841029', 't_vz_01841029', 'c_vz_56846900'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deixando os dados contínuos, numa base diária.\n",
    "# df = df.resample('D').first()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando os dados finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Neste momento, tenho o DataFrame com os dados EXATAMENTE da forma que preciso.\n",
    "# # Posso, inclusive, exportar isso para um arquivo de Excel\n",
    "# # É o que farei, pois se precisar retornar aos dados originais, será mais fácil que fazer toda engenharia até aqui\n",
    "\n",
    "# df.to_excel('./arquivos_finais/medio_rio_doce_final.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixo Rio Jequitinhonha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando os dados das estações que serão utilizadas no trabalho\n",
    "# As estações foram selecionadas a partir do sistema Data Rhama\n",
    "# Aqui eu baixo os dados e salvo localmente\n",
    "# >>>>>>>>>>>>> SÓ PRECISA FAZER ISSO UMA VEZ, POR ISSO O CÓDIGO FICA COMENTADO DEPOIS DE RODAR!!!! <<<<<<<<<<<<<\n",
    "\n",
    "estacao_principal = '54790000'\n",
    "outras_estacoes = ['54780000', '01640007', '01640000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemétricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4823it [00:03, 1513.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Code</th>\n",
       "      <th>Status</th>\n",
       "      <th>SubBasin</th>\n",
       "      <th>City-State</th>\n",
       "      <th>Origem</th>\n",
       "      <th>Responsible</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RIO PRETO DA EVA</td>\n",
       "      <td>00259004</td>\n",
       "      <td>Ativo</td>\n",
       "      <td>15</td>\n",
       "      <td>RIO PRETO DA EVA-AM</td>\n",
       "      <td>Açudes Semiárido</td>\n",
       "      <td>00001 - ANA - Agência Nacional de Águas</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.7003</td>\n",
       "      <td>-59.6997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UHE BELO MONTE BR230</td>\n",
       "      <td>00351004</td>\n",
       "      <td>Ativo</td>\n",
       "      <td>18</td>\n",
       "      <td>VITÓRIA DO XINGU-PA</td>\n",
       "      <td>Setor Elétrico</td>\n",
       "      <td>00594 - NORTE ENERGIA - Norte Energia S.A</td>\n",
       "      <td>33.00</td>\n",
       "      <td>-3.1267</td>\n",
       "      <td>-51.7906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UHE BELO MONTE SÍTIO PIMENTAL</td>\n",
       "      <td>00351005</td>\n",
       "      <td>Ativo</td>\n",
       "      <td>18</td>\n",
       "      <td>VITÓRIA DO XINGU-PA</td>\n",
       "      <td>Setor Elétrico</td>\n",
       "      <td>00594 - NORTE ENERGIA - Norte Energia S.A</td>\n",
       "      <td>110.00</td>\n",
       "      <td>-3.3758</td>\n",
       "      <td>-51.9403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UHE BELO MONTE VISTA ALEGRE</td>\n",
       "      <td>00352009</td>\n",
       "      <td>Ativo</td>\n",
       "      <td>18</td>\n",
       "      <td>ALTAMIRA-PA</td>\n",
       "      <td>Setor Elétrico</td>\n",
       "      <td>00594 - NORTE ENERGIA - Norte Energia S.A</td>\n",
       "      <td>125.00</td>\n",
       "      <td>-3.1186</td>\n",
       "      <td>-52.2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UHE BELO MONTE SÃO FRANCISCO</td>\n",
       "      <td>00352010</td>\n",
       "      <td>Ativo</td>\n",
       "      <td>18</td>\n",
       "      <td>ALTAMIRA-PA</td>\n",
       "      <td>Setor Elétrico</td>\n",
       "      <td>00594 - NORTE ENERGIA - Norte Energia S.A</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-3.2533</td>\n",
       "      <td>-52.3489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name      Code Status SubBasin  \\\n",
       "1               RIO PRETO DA EVA  00259004  Ativo       15   \n",
       "2           UHE BELO MONTE BR230  00351004  Ativo       18   \n",
       "3  UHE BELO MONTE SÍTIO PIMENTAL  00351005  Ativo       18   \n",
       "4    UHE BELO MONTE VISTA ALEGRE  00352009  Ativo       18   \n",
       "5   UHE BELO MONTE SÃO FRANCISCO  00352010  Ativo       18   \n",
       "\n",
       "            City-State            Origem  \\\n",
       "1  RIO PRETO DA EVA-AM  Açudes Semiárido   \n",
       "2  VITÓRIA DO XINGU-PA    Setor Elétrico   \n",
       "3  VITÓRIA DO XINGU-PA    Setor Elétrico   \n",
       "4          ALTAMIRA-PA    Setor Elétrico   \n",
       "5          ALTAMIRA-PA    Setor Elétrico   \n",
       "\n",
       "                                 Responsible Elevation  Latitude  Longitude  \n",
       "1   00001 - ANA - Agência Nacional de Águas       0.00   -2.7003   -59.6997  \n",
       "2  00594 - NORTE ENERGIA - Norte Energia S.A     33.00   -3.1267   -51.7906  \n",
       "3  00594 - NORTE ENERGIA - Norte Energia S.A    110.00   -3.3758   -51.9403  \n",
       "4  00594 - NORTE ENERGIA - Norte Energia S.A    125.00   -3.1186   -52.2525  \n",
       "5  00594 - NORTE ENERGIA - Norte Energia S.A    124.00   -3.2533   -52.3489  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a lib HydroBR eu desejo saber se as estações em questão são do tipo convencional ou telemétrica\n",
    "# O código não exclui o fato, eventual, de uma dada estação ser convencional E telemétrica, como é o caso aqui\n",
    "\n",
    "lista_estacoes = hbr.get_data.ANA.list_telemetric() # Vendo primeiro se tem telemétrica\n",
    "lista_estacoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estação 54790000 -> True\n"
     ]
    }
   ],
   "source": [
    "# Averiguando se as estações que tenho em mãos estão presentes neste conjunto de estações telemétricas\n",
    "\n",
    "print(\"Estação {e} -> {p}\".format(\n",
    "                                e=estacao_principal,\n",
    "                                p=(lista_estacoes['Code'] == estacao_principal).any()\n",
    "                            )\n",
    ")\n",
    "\n",
    "# A estação principal em questão tem dados telemétricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estação 54780000 -> False\n",
      "Estação 01640007 -> False\n",
      "Estação 01640000 -> False\n"
     ]
    }
   ],
   "source": [
    "# Verificando as outras estações\n",
    "\n",
    "for e in outras_estacoes:\n",
    "    print(\"Estação {e} -> {p}\".format(\n",
    "                                        e=e,\n",
    "                                        p=(lista_estacoes['Code'] == e).any()\n",
    "                                    )\n",
    "    )\n",
    "\n",
    "# Estas estações também têm dados telemétricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "syntax error: line 1, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\welson\\anaconda3\\envs\\dissertacao_py39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3550\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[33], line 3\u001b[0m\n    gerar_dados_tele(\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 167\u001b[0m in \u001b[0;35mgerar_dados_tele\u001b[0m\n    df_result = get_telemetrica(codEstacao=estacao_principal, dataInicio=dt_inicio, dataFim=dt_fim)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 26\u001b[0m in \u001b[0;35mget_telemetrica\u001b[0m\n    tree = ET.ElementTree(ET.fromstring(response.content))\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\welson\\anaconda3\\envs\\dissertacao_py39\\lib\\xml\\etree\\ElementTree.py:1347\u001b[1;36m in \u001b[1;35mXML\u001b[1;36m\n\u001b[1;33m    parser.feed(text)\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m syntax error: line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "# Gerando um arquivo Excel com os dados das estações telemétricas\n",
    "\n",
    "gerar_dados_tele(\n",
    "    estacao_principal=estacao_principal,\n",
    "    outras_estacoes=None,\n",
    "    nome_arq=\"baixo_rio_jequitinhonha\",\n",
    "    dt_inicio='2013-01-01',\n",
    "    dt_fim='2023-12-31',\n",
    "    salvar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convencionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cota/Vazão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a lib HydroBR eu desejo saber se as estações em questão são do tipo convencional ou telemétrica\n",
    "# O código não exclui o fato, eventual, de uma dada estação ser convencional E telemétrica, como é o caso aqui\n",
    "\n",
    "lista_estacoes = hbr.get_data.ANA.list_flow( # Verificando se tem estações de cota/vazão primeiro\n",
    "    state='MINAS GERAIS',\n",
    "    source='ANA'\n",
    ")\n",
    "\n",
    "lista_estacoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averiguando se as estações que tenho em mãos estão presentes neste conjunto de estações convencionais de cota/vazão\n",
    "\n",
    "print(\n",
    "    \"Estação {e} -> {p}\".format(\n",
    "        e=estacao_principal,\n",
    "        p=(lista_estacoes['Code'] == estacao_principal).any()\n",
    "    )\n",
    ")\n",
    "\n",
    "# A estação principal tem dados convencionais de cota/vazão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as outras estações\n",
    "\n",
    "for e in outras_estacoes:\n",
    "    print(\n",
    "        \"Estação {e} -> {p}\".format(\n",
    "            e=e,\n",
    "            p=(lista_estacoes['Code'] == e).any()\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Estas estações também têm dados convencionais de cota/vazão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um arquivo Excel com os dados das estações convencionais\n",
    "\n",
    "estacoes_conv = [\"56989400\", \"56989900\", \"56990000\", \"56990850\", \"56990005\"]\n",
    "gerar_dados_conv(\n",
    "    estacao_principal=estacao_principal,\n",
    "    outras_estacoes=estacoes_conv,\n",
    "    nome_arq=\"baixo_rio_doce\",\n",
    "    dt_inicio='2013-01-01',\n",
    "    dt_fim='2023-12-31',\n",
    "    tp_dados=1, # Cota\n",
    "    nvl_consistencia='2', # dados consistidos\n",
    "    salvar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um arquivo Excel com os dados das estações convencionais\n",
    "\n",
    "gerar_dados_conv(\n",
    "    estacao_principal=estacao_principal,\n",
    "    outras_estacoes=estacoes_conv,\n",
    "    nome_arq=\"baixo_rio_doce\",\n",
    "    dt_inicio='2013-01-01',\n",
    "    dt_fim='2023-12-31',\n",
    "    tp_dados=3, # Vazão\n",
    "    nvl_consistencia='2', # dados consistidos\n",
    "    salvar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_estacoes = hbr.get_data.ANA.list_prec(\n",
    "    state='MINAS GERAIS',\n",
    "    source='ANA'\n",
    ")\n",
    "\n",
    "lista_estacoes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Estação {e} -> {p}\".format(\n",
    "        e=estacao_principal,\n",
    "        p=(lista_estacoes['Code'] == estacao_principal).any()\n",
    "    )\n",
    ")\n",
    "\n",
    "# A estação principal NÃO tem dados convencionais de chuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando as outras estações\n",
    "\n",
    "for e in outras_estacoes:\n",
    "    print(\n",
    "        \"Estação {e} -> {p}\".format(\n",
    "            e=e,\n",
    "            p=(lista_estacoes['Code'] == e).any()\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Estas estações têm dados convencionais de chuva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um arquivo Excel com os dados das estações convencionais\n",
    "\n",
    "estacoes_conv = [\"01941010\", \"01941004\", \"01941006\"]\n",
    "gerar_dados_conv(\n",
    "    estacao_principal=estacao_principal,\n",
    "    outras_estacoes=estacoes_conv,\n",
    "    nome_arq=\"baixo_rio_doce\",\n",
    "    dt_inicio='2013-01-01',\n",
    "    dt_fim='2023-12-31',\n",
    "    tp_dados=2, # Chuva\n",
    "    nvl_consistencia='2', # dados consistidos\n",
    "    salvar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = ['baixo_rio_doce_dados_chuva_conv.xlsx', 'baixo_rio_doce_dados_tele.xlsx', 'baixo_rio_doce_dados_vazao_conv.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vou fazer a carga primeiro dos dados telemétricos, porque é onde tem mais informação de uma única vez.\n",
    "# Depois concateno os outros arquivos. Mas a ordem tanto faz aqui, só estipulei assim porque acho melhor\n",
    "\n",
    "df = pd.read_excel(arquivos[0], sheet_name=0, index_col=0, header=0, parse_dates=['Data'])\n",
    "\n",
    "for a in range(1, len(arquivos)):\n",
    "    df_temp = pd.read_excel(arquivos[a], sheet_name=0, index_col=0, header=0, parse_dates=['Data'])\n",
    "    df = pd.concat([df, df_temp], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vou remover as colunas das cotas\n",
    "\n",
    "df = df.drop(columns=['t_ct_56994500', 't_ct_56990850', 't_ct_56990005'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o merge das colunas de vazão que são correspondentes à mesma estação\n",
    "# Acontece que existem gaps entre os dados, o que é estranho, porque a estação telemétrica tem dados que a convencional não tem e vice-versa.\n",
    "# Vou contar qual coluna tem mais dados e depois executar um 'fillna'\n",
    "\n",
    "colunas_esquerda = ['t_vz_56994500', 't_vz_56990850', 't_vz_56990005']\n",
    "colunas_direita = ['c_vz_56994500', 'c_vz_56990850', 'c_vz_56990005']\n",
    "\n",
    "for i, j in zip(colunas_esquerda, colunas_direita):\n",
    "    print(i, j, df[i].isna().sum(), df[j].isna().sum())\n",
    "\n",
    "# A coluna que tiver menos buracos será preenchida com os dados da coluna que tem mais dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preenchendo a coluna que tem menos dados faltantes com a outra correspondente\n",
    "# Fazer isso para cada coluna, contudo, tem colunas que tem dados faltando demais. Neste caso, darei drop nelas inteiramente.\n",
    "df['c_vz_56994500'].fillna(df['t_vz_56994500'], inplace=True)\n",
    "df['t_vz_56990850'].fillna(df['c_vz_56990850'], inplace=True)\n",
    "df['t_vz_56990005'].fillna(df['c_vz_56990005'], inplace=True)\n",
    "\n",
    "df['c_vz_56994500'].isna().sum(), df['t_vz_56990850'].isna().sum(), df['t_vz_56990005'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as colunas que foram usadas para preencher os vazios\n",
    "\n",
    "df = df.drop(columns=[\"t_vz_56994500\", \"c_vz_56990850\", \"c_vz_56990005\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a quantidade de valores \"NaN\" em cada coluna\n",
    "\n",
    "print(\"Quantidade de NaN por coluna\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"Percentual de NaN por coluna\")\n",
    "for c in np.asarray(df.columns):\n",
    "    print(c, (df[c].isna().sum()/len(df))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixando os dados contínuos, numa base diária.\n",
    "df = df.resample('D').first()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste momento, tenho o DataFrame com os dados EXATAMENTE da forma que preciso.\n",
    "# Posso, inclusive, exportar isso para um arquivo de Excel\n",
    "# É o que farei, pois se precisar retornar aos dados originais, será mais fácil que fazer toda engenharia até aqui\n",
    "\n",
    "df.to_excel('baixo_rio_doce_final.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertacao_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
