{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports básicos para todas as análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desativar as mensagens de 'warning' que ficam poluindo o output aqui no Colab\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações dos módulos necessários para autenticar na minha conta Google e acessar o Drive\n",
    "import  pandas as pd,               \\\n",
    "        numpy as np,                \\\n",
    "        matplotlib.pyplot as plt,   \\\n",
    "        requests as rt,             \\\n",
    "        seaborn as sns,             \\\n",
    "        xml.etree.ElementTree as ET\n",
    "\n",
    "from io               import BytesIO\n",
    "from tqdm             import tqdm\n",
    "from matplotlib.pylab import rcParams\n",
    "from pandas.plotting  import register_matplotlib_converters\n",
    "\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.compose import make_reduction\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "\n",
    "# Ajustes feitos para geração e criação de gráfico\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "# Tratar conversões de DateTime entre o Pandas e o Matplotlib\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import calendar\n",
    "# import datetime\n",
    "# import json\n",
    "# import os\n",
    "# import requests\n",
    "# import xml.etree.ElementTree as ET\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def get_data_telemetrica(codEstacao, dataInicio, dataFim): #, save=False\n",
    "\n",
    "#     # 1. Fazer a requisião ao servidor e pegar a árvore e a raiz dos dados\n",
    "#     params = {'codEstacao':codEstacao, 'dataInicio':dataInicio, 'dataFim':dataFim}\n",
    "#     server = 'http://telemetriaws1.ana.gov.br/ServiceANA.asmx/DadosHidrometeorologicos'\n",
    "#     response = requests.get(server, params)\n",
    "#     tree = ET.ElementTree(ET.fromstring(response.content))\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     # 2. Iteração dentro dos elementos do XML procurando os dados que são disponibilizados para a estação\n",
    "#     list_vazao = []\n",
    "#     list_data = []\n",
    "#     list_nivel = []\n",
    "#     list_chuva = []\n",
    "\n",
    "#     for i in tqdm(root.iter('DadosHidrometereologicos')):\n",
    "\n",
    "#         data = i.find('DataHora').text\n",
    "#         try:\n",
    "#             vazao = float(i.find('Vazao').text)\n",
    "#         except TypeError:\n",
    "#             vazao = i.find('Vazao').text\n",
    "\n",
    "#         try:\n",
    "#             nivel = float(i.find('Nivel').text)\n",
    "#         except TypeError:\n",
    "#             nivel = i.find('Nivel').text\n",
    "\n",
    "#         try:\n",
    "#             chuva = float(i.find('Chuva').text)\n",
    "#         except TypeError:\n",
    "#             chuva = i.find('Chuva').text\n",
    "\n",
    "#         list_vazao.append(vazao)\n",
    "#         list_data.append(data)\n",
    "#         list_nivel.append(nivel)\n",
    "#         list_chuva.append(chuva)\n",
    "\n",
    "#     df = pd.DataFrame([list_data, list_nivel, list_chuva, list_vazao]).transpose()\n",
    "#     df.columns = ['Data', 'Nivel', 'Chuva', 'Vazao']\n",
    "\n",
    "#     df = df.sort_values(by='Data')\n",
    "#     df = df.set_index('Data')\n",
    "#     df.index = pd.to_datetime(df.index)\n",
    "\n",
    "#     # if save == True:\n",
    "#     #     df.to_excel(codEstacao+'_dados.xlsx')\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data, n_lags=1, n_targets=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tRearranja a série temporal como um dataset para aprendizado supervisionado.\n",
    "\tArgumentos:\n",
    "\t\tdata: Sequência de observações. Pode ser list ou NumPy array\n",
    "\t\tn_lags: Número de lags das observações que serão usadas como input (X).\n",
    "\t\tn_targets: Número de observações que serão usadas como output (y).\n",
    "\t\tdropnan: Booleano pra dropar as linhas que ficarem com NaN .\n",
    "\tRetorna:\n",
    "\t\tUm DataFrame como uma série ajustada para aprendizado supervisionado.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\n",
    "\t# As lags (t-n, ... t-1)\n",
    "\tfor i in range(n_lags, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('X_%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "\t# Prever uma sequência (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_targets):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('y_%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('y_%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "\t# Junta tudo\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\n",
    "\t# Dropa as linhas com NaN\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\n",
    "\treturn agg # IMPORTANTE!!!! O retorno sai com as colunas com nome zuado. Tem que renomear o DataFrame resultante depois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O corte realizado por esse método é simples: pega do início da série até train_size e coloca num dataframe para treino\n",
    "# O que resta do corte anterior, é colocado no dataframe para teste\n",
    "def split_train_test(df, train_size=0.7):\n",
    "\n",
    "  # Tamanho total da série\n",
    "  size = df.shape[0]\n",
    "\n",
    "  # Tamanho do treino\n",
    "  t_size = int(size * train_size)\n",
    "\n",
    "  train_data = df.iloc[0:t_size]\n",
    "  validation_data = df.iloc[t_size:]\n",
    "\n",
    "  return train_data, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Médio Jequitinhonha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o objeto 'dataset' para trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "estacao_principal = \"./estacoes_medio/planilhas_ajustadas/principal_vazao_54195000.csv\"\n",
    "df_left = pd.read_csv(estacao_principal, sep='\\t', index_col=0, header=0, parse_dates=['data'])\n",
    "\n",
    "outras_estacoes = [\"./estacoes_medio/planilhas_ajustadas/chuva_1642007.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/chuva_1642008.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/chuva_1642027.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/cota_54150000.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/cota_54193000.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/cota_54195000.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/telemetric_1642041.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/telemetric_54140000.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/telemetric_54150001.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/vazao_54140000.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/vazao_54150000.csv\",\n",
    "                   \"./estacoes_medio/planilhas_ajustadas/vazao_54193000.csv\"]\n",
    "\n",
    "df_list = []\n",
    "for f in outras_estacoes:\n",
    "    df_list.append(pd.read_csv(f, sep='\\t', index_col=0, header=0, parse_dates=['data']))\n",
    "\n",
    "# df_merged = pd.merge(df_principal, df_secundaria, how=\"left\", on=\"data\", suffixes=[None, '_r'])\n",
    "\n",
    "# df_merged[df_merged.index.year == 2015].to_clipboard(sep='\\t')\n",
    "# df_merged.dropna()\n",
    "    \n",
    "# df_left.to_clipboard(sep='\\t')\n",
    "\n",
    "# df_left.merge(df_right, how='left', on='data')\n",
    "    \n",
    "for df in df_list:\n",
    "    df_result = df_left.merge(df, how='left', on='data', suffixes=(None, '_r'))\n",
    "    df_left = df_result\n",
    "\n",
    "df_result.to_clipboard(sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_dataset(estacao_principal, outras_estacoes):\n",
    "    df_principal = pd.read_csv(estacao_principal, sep='\\t', index_col=0, header=0, parse_dates=['data'])\n",
    "    \n",
    "    df_outras = []\n",
    "    for f in outras_estacoes:\n",
    "        df_outras.append(pd.read_csv(f, sep='\\t', index_col=0, header=0, parse_dates=['data']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixo Jequitinhonha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralFC_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
