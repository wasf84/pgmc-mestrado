{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df37755d-bdc2-4602-9863-5b7bc0444b91",
   "metadata": {},
   "source": [
    "### Preparação do ambiente\n",
    "\n",
    "Este projeto demanda a instalação dos pacotes **numpy, matplotlib e gym[all]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc999fe-5d11-408c-a7a6-ee6ff5eab0df",
   "metadata": {},
   "source": [
    "### Preâmbulo do projeto\n",
    "\n",
    "Importação de todos pacotes necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68d5797-5aa7-49c0-b01b-70a3ba2c2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, math, tqdm, numpy as np, matplotlib.pyplot as plt\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7c93e-a9f3-4013-9eb1-c9dce7083e9e",
   "metadata": {},
   "source": [
    "### Inicialização do ambiente e das variáveis\n",
    "\n",
    "Nos exercícios anteriores, CartPole e MountainCar, as ações eram um valor discreto sempre. No caso do CartoPole era empurrar pra esquerda ou pra direita. No MountainCar era empurrar pra esquerda, pra direita ou não empurrar. Não havia como \"dosar\" o empurrão, ou tinha, ou não tinha.\n",
    "\n",
    "No caso do Pendulum, só tem uma ação, porém, essa ação é um valor entre -2,0 e 2,0, ou seja, é um valor contínuo dentro do intervalo. Precisa \"dosar\" esse torque porque senão o pêndulo pode nunca convergir para a posição de repouso, a saber, de pé a 90 graus. Com a variável \"size_ac\" escolhe-se o tamanho da discretização deste intervalo -2,0 a 2,0. Claro que um número maior permitirá um ajuste mais afinado do modelo, mas também consumirá mais recurso computacional, ocupando memória. A variável \"discrete_ac\" é o vetor onde vai colocar cada valor para o torque. Por exemplo, suponhamos um \"size_ac\" de tamanho 3. Neste caso, \"discrete_ac[3]\" armazenará os valores \"-2 0 2\". Este vetor é empregado no método \"discrete_action()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a793aa3-57aa-4c2c-a7d7-7729d9d059ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v1\"\n",
    "env = gym.make(env_name)\n",
    "num_eps = 3000\n",
    "epsilon, min_epsilon = 0.0, 0.1\n",
    "learning_rate, min_learning_rate = 0.0, 0.5\n",
    "discount_factor = 0.95\n",
    "dec = 15\n",
    "\n",
    "# Essa variável é apenas para discretizar a observação do ambiente.\n",
    "discrete_st = [20, 20, 20]\n",
    "\n",
    "# Estas variáveis são usadas para discretização do torque.\n",
    "size_ac = 20\n",
    "discrete_ac = [size_ac]\n",
    "lower_st = [-1.0, -1.0, -8.0]\n",
    "upper_st = [1.0, 1.0, 8.0]\n",
    "lower_ac = [-2.0]\n",
    "upper_ac = [2.0]\n",
    "\n",
    "# A QTable neste problema será uma composição entre 2 listas.\n",
    "# A lista com as observações do ambiente e outra lista dos torques q serem empregados no pêndulo\n",
    "qtable = np.zeros(discrete_st + discrete_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a658ce0-fb1f-4a62-82ce-ab5ae6273b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTE\n",
    "\n",
    "#np.random.randint(size_ac)], [np.argmax(qtable[discrete_state(env.reset())])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75018b2f-22a8-4b18-945f-b5f0a2e9216c",
   "metadata": {},
   "source": [
    "### Métodos que serão empregados na geração da QTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b5fa58-a541-446a-aa43-fdf6e9db7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    \"\"\"\n",
    "        O fator aleatório para melhorar o aprendizado\n",
    "    \"\"\"\n",
    "    if (np.random.random() < epsilon):\n",
    "        # Aqui uma alteração importante em relação aos trabalhos realizados anteriormente\n",
    "        #    retornará um número aleatório de 0 até 'size_ac' que então será buscado no 'discrete_ac'.\n",
    "        return [np.random.randint(size_ac)]\n",
    "    else:\n",
    "        return [np.argmax(qtable[state])]\n",
    "# ---------------------------------------------------------------------#\n",
    "def discrete_state(state):\n",
    "    \"\"\"\n",
    "        Recebe um espaço de observação (observation space) e o discretiza\n",
    "    \"\"\"\n",
    "    d = list()\n",
    "\n",
    "    for i in range(len(state)):\n",
    "        stepsize = (state[i] + abs(lower_st[i])) / (upper_st[i] - lower_st[i])\n",
    "        new_state = int(round((discrete_st[i] - 1) * stepsize))\n",
    "        new_state = min(discrete_st[i] - 1, max(0, new_state))\n",
    "        d.append(new_state)\n",
    "\n",
    "    return tuple(d)\n",
    "# ---------------------------------------------------------------------#\n",
    "def discrete_action(action):\n",
    "    \"\"\"\n",
    "        Recebe ações (action space) e o discretiza\n",
    "    \"\"\"\n",
    "    d = list()\n",
    "\n",
    "    for i in range(len(action)):\n",
    "        stepsize = (action[i] + abs(lower_ac[i])) / (upper_ac[i] - lower_ac[i])\n",
    "        new_action = int(round((discrete_ac[i] - 1) * stepsize))\n",
    "        new_action = min(discrete_ac[i] - 1, max(0, new_action))\n",
    "        d.append(new_action)\n",
    "\n",
    "    return tuple(d)\n",
    "# ---------------------------------------------------------------------#\n",
    "def continuous_action(idx):\n",
    "    \"\"\"\n",
    "        Recebe um índice que retornara a posição do vetor \"discrete_ac\" com o valor contínuo da ação naquela posição.\n",
    "    \"\"\"\n",
    "    d = list()\n",
    "\n",
    "    for i in range(len(idx)):\n",
    "        stepsize = idx[i] / (discrete_ac[i] - 1)\n",
    "        new_action = lower_ac[i] + stepsize * (upper_ac[i] - lower_ac[i])\n",
    "        d.append(new_action)\n",
    "\n",
    "    return tuple(d)\n",
    "# ---------------------------------------------------------------------#\n",
    "def get_epsilon(ep):\n",
    "    # vai reduzindo o epsilon para diminuir a ganância do algoritmo\n",
    "    # entretando, existe um valor mínimo que não reduz para garantir alguma aleatoriedade\n",
    "    # reduz à medida que aumenta a quantidade de episódios\n",
    "    # https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "    return max(min_epsilon, min(1., 1. - math.log10((ep + 1) / dec)))\n",
    "# ---------------------------------------------------------------------#\n",
    "def get_learning_rate(ep):\n",
    "    # vai reduzindo a taxa de aprendizagem para diminuir a ganância do algoritmo\n",
    "    # entretando, existe um valor mínimo que não reduz para garantir algum aprendizado\n",
    "    # reduz à medida que aumenta a quantidade de episódios\n",
    "    # https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "    return max(min_learning_rate, min(1., 1. - math.log10((ep + 1) / dec)))\n",
    "# ---------------------------------------------------------------------#\n",
    "def update_qtable(state, action, reward, new_state):\n",
    "    # isso está no Google Colab da disciplina\n",
    "    qtable[state][action] += learning_rate * (reward + discount_factor * np.max(qtable[new_state]) - qtable[state][action])\n",
    "# ---------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a52274-3c63-4a02-afd3-0967bc24cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTE\n",
    "\n",
    "#test = discrete_action([1.9])\n",
    "#print(test)\n",
    "\n",
    "#for x in np.linspace(-2, 2, 10):\n",
    "#    print(x, continuous_action(discrete_action([x])))\n",
    "\n",
    "#env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0332dc0-24eb-4a4c-8ba5-15dc8ca54dbd",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297b7c43-0c1b-42c7-9195-bfc3e1afc597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 4238/30000 [01:15<07:36, 56.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m continuous_action(idx_disc_ac)\n\u001b[1;32m     17\u001b[0m observation, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 18\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscrete_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m update_qtable(current_state, idx_disc_ac, reward, new_state)\n\u001b[1;32m     20\u001b[0m current_state \u001b[38;5;241m=\u001b[39m new_state\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mdiscrete_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     16\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state)):\n\u001b[0;32m---> 19\u001b[0m     stepsize \u001b[38;5;241m=\u001b[39m (\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlower_st\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (upper_st[i] \u001b[38;5;241m-\u001b[39m lower_st[i])\n\u001b[1;32m     20\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((discrete_st[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m stepsize))\n\u001b[1;32m     21\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(discrete_st[i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, new_state))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# usado para plotar o gráfico de aprendizado\n",
    "tr = []\n",
    "\n",
    "for ep in tqdm.tqdm(range(num_eps)):\n",
    "    current_state = discrete_state(env.reset())\n",
    "\n",
    "    learning_rate = get_learning_rate(ep)\n",
    "    epsilon = get_epsilon(ep)\n",
    "    done = False\n",
    "\n",
    "    # usado para plotar o gráfico de aprendizado\n",
    "    r = []\n",
    "\n",
    "    while not done:\n",
    "        idx_disc_ac = choose_action(current_state)\n",
    "        action = continuous_action(idx_disc_ac)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        new_state = discrete_state(observation)\n",
    "        update_qtable(current_state, idx_disc_ac, reward, new_state)\n",
    "        current_state = new_state\n",
    "        r.append(reward)\n",
    "\n",
    "    env.close()\n",
    "    tr.append(sum(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d752f15-f915-4379-9765-3881e8af121a",
   "metadata": {},
   "source": [
    "### Gráfico da aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf290f7c-9373-43dd-9a88-a68148bc43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5edbe6-9656-4e40-a5fb-33f472a3b58d",
   "metadata": {},
   "source": [
    "### Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12050631-7c71-4916-b56f-28ab9ff5935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = []     # usado para plotar o histograma\n",
    "i = 0       # macete para renderizar alguns episodios\n",
    "\n",
    "# executando sempre para 10% de passos sobre o número total de episódios usados para treinamento\n",
    "for _ in tqdm.tqdm(range(int(num_eps * 0.1))):\n",
    "    \n",
    "    # ambiente a ser avaliado\n",
    "    env_t = gym.make(env_name)\n",
    "    \n",
    "    # usado para plotar o gráfico de aprendizado\n",
    "    r = []\n",
    "\n",
    "    done = False\n",
    "\n",
    "    # estado atual\n",
    "    cs = discrete_state(env_t.reset())\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        # Renderizar os ultimos 5 episodios\n",
    "        if i >= (int(num_eps * 0.1) - 5):\n",
    "            env_t.render()\n",
    "\n",
    "        disc_ac = [np.argmax(qtable[cs])]\n",
    "        action = continuous_action(disc_ac)\n",
    "        obs, reward, done, _ = env_t.step(action)\n",
    "        new_state = discrete_state(obs)\n",
    "        cs = new_state\n",
    "        r.append(reward)\n",
    "        \n",
    "    i += 1\n",
    "    env_t.close()\n",
    "    tr.append(sum(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719bc4a-171f-420f-8c1c-55d3f5896f36",
   "metadata": {},
   "source": [
    "### Histograma da execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde2a81-c0ee-4f4d-b154-c07a17e5bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
